{
  "components": {
    "comp-ingest-data": {
      "executorLabel": "exec-ingest-data",
      "inputDefinitions": {
        "artifacts": {
          "input_table": {
            "artifactType": {
              "schemaTitle": "google.BQTable",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "ingestion_batch_size": {
            "parameterType": "NUMBER_INTEGER"
          },
          "is_incremental": {
            "defaultValue": true,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "location": {
            "parameterType": "STRING"
          },
          "look_back_days": {
            "defaultValue": 1.0,
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "project_id": {
            "description": "Google Cloud project ID",
            "parameterType": "STRING"
          },
          "schedule_time": {
            "parameterType": "STRING"
          },
          "vector_search_data_bucket_name": {
            "parameterType": "STRING"
          },
          "vector_search_index": {
            "parameterType": "STRING"
          },
          "vector_search_index_endpoint": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-process-data": {
      "executorLabel": "exec-process-data",
      "inputDefinitions": {
        "parameters": {
          "chunk_overlap": {
            "defaultValue": 20.0,
            "description": "Overlap between chunks",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "chunk_size": {
            "defaultValue": 1500.0,
            "description": "Size of text chunks",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "deduped_table": {
            "defaultValue": "questions_embeddings",
            "description": "Table for storing deduplicated results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_dataset": {
            "defaultValue": "stackoverflow_data",
            "description": "BigQuery dataset for storing results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_table": {
            "defaultValue": "incremental_questions_embeddings",
            "description": "Table for storing incremental results",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "is_incremental": {
            "defaultValue": true,
            "description": "Whether to process only recent data",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "location": {
            "defaultValue": "us-central1",
            "description": "BigQuery location",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "look_back_days": {
            "defaultValue": 1.0,
            "description": "Number of days to look back for incremental processing",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "schedule_time": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "output_table": {
            "artifactType": {
              "schemaTitle": "google.BQTable",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-ingest-data": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "ingest_data"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import BQTable\n\ndef ingest_data(\n    project_id: str,\n    location: str,\n    vector_search_index: str,\n    vector_search_index_endpoint: str,\n    vector_search_data_bucket_name: str,\n    schedule_time: str,\n    ingestion_batch_size: int,\n    input_table: Input[BQTable],\n    is_incremental: bool = True,\n    look_back_days: int = 1,\n) -> None:\n    \"\"\"Process and ingest documents into Vertex AI Vector Search.\n\n    Args:\n        project_id: Google Cloud project ID\n    \"\"\"\n    import logging\n    from datetime import datetime, timedelta\n\n    import bigframes.pandas as bpd\n    from google.cloud import aiplatform\n    from langchain_google_vertexai import VectorSearchVectorStore\n    from langchain_google_vertexai import VertexAIEmbeddings\n\n    # Initialize logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Initialize clients\n    logging.info(\"Initializing clients...\")\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    logging.info(\"Clients initialized.\")\n\n    # Set date range for data fetch\n    schedule_time_dt: datetime = datetime.fromisoformat(\n        schedule_time.replace(\"Z\", \"+00:00\")\n    )\n    if schedule_time_dt.year == 1970:\n        logging.warning(\n            \"Pipeline schedule not set. Setting schedule_time to current date.\"\n        )\n        schedule_time_dt = datetime.now()\n\n    # Note: The following line sets the schedule time 5 years back to allow sample data to be present.\n    # For your use case, please comment out the following line to use the actual schedule time.\n    schedule_time_dt = schedule_time_dt - timedelta(days=5 * 365)\n\n    START_DATE: datetime = schedule_time_dt - timedelta(\n        days=look_back_days\n    )  # Start date for data processing window\n    END_DATE: datetime = schedule_time_dt  # End date for data processing window\n\n    logging.info(f\"Date range set: START_DATE={START_DATE}, END_DATE={END_DATE}\")\n\n    dataset = input_table.metadata[\"datasetId\"]\n    table = input_table.metadata[\"tableId\"]\n\n    query = f\"\"\"\n        SELECT\n            question_id\n            , last_edit_date\n            , full_text_md\n            , text_chunk\n            , chunk_id\n            , embedding\n        FROM  {project_id}.{dataset}.{table}\n        WHERE TRUE\n                {f'AND DATETIME(creation_timestamp) BETWEEN DATETIME(\"{START_DATE}\") AND DATETIME(\"{END_DATE}\")' if is_incremental else \"\"}\n    \"\"\"\n    df = (\n        bpd.read_gbq(query)\n        .sort_values(\"last_edit_date\", ascending=False)\n        .drop_duplicates(\"question_id\")\n        .reset_index(drop=True)\n    )\n\n    aiplatform.init(\n        project=project_id,\n        location=location,\n        staging_bucket=vector_search_data_bucket_name,\n    )\n\n    embedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n    my_index = aiplatform.MatchingEngineIndex(vector_search_index)\n    my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n        vector_search_index_endpoint\n    )\n    vector_store = VectorSearchVectorStore.from_components(\n        project_id=project_id,\n        region=location,\n        gcs_bucket_name=vector_search_data_bucket_name.replace(\"gs://\", \"\"),\n        index_id=my_index.name,\n        endpoint_id=my_index_endpoint.name,\n        embedding=embedding_model,\n        stream_update=True,\n    )\n\n    for batch_num, start in enumerate(range(0, len(df), ingestion_batch_size)):\n        ids = (\n            df.iloc[start : start + ingestion_batch_size]\n            .question_id.astype(str)\n            .tolist()\n        )\n        texts = df.iloc[start : start + ingestion_batch_size].text_chunk.tolist()\n        embeddings = df.iloc[start : start + ingestion_batch_size].embedding.tolist()\n        metadatas = (\n            df.iloc[start : start + ingestion_batch_size]\n            .drop(columns=[\"embedding\", \"last_edit_date\"])\n            .to_dict(orient=\"records\")\n        )\n        vector_store.add_texts_with_embeddings(\n            ids=ids,\n            texts=texts,\n            embeddings=embeddings,\n            metadatas=metadatas,\n            is_complete_overwrite=True,\n        )\n\n"
          ],
          "image": "us-docker.pkg.dev/production-ai-template/starter-pack/data_processing:0.2"
        }
      },
      "exec-process-data": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "process_data"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import BQTable\n\ndef process_data(\n    project_id: str,\n    schedule_time: str,\n    output_table: Output[BQTable],\n    is_incremental: bool = True,\n    look_back_days: int = 1,\n    chunk_size: int = 1500,\n    chunk_overlap: int = 20,\n    destination_dataset: str = \"stackoverflow_data\",\n    destination_table: str = \"incremental_questions_embeddings\",\n    deduped_table: str = \"questions_embeddings\",\n    location: str = \"us-central1\",\n) -> None:\n    \"\"\"Process StackOverflow questions and answers by:\n    1. Fetching data from BigQuery\n    2. Converting HTML to markdown\n    3. Splitting text into chunks\n    4. Generating embeddings\n    5. Storing results in BigQuery\n    6. Exporting to JSONL\n\n    Args:\n        output_files: Output dataset path\n        is_incremental: Whether to process only recent data\n        look_back_days: Number of days to look back for incremental processing\n        chunk_size: Size of text chunks\n        chunk_overlap: Overlap between chunks\n        destination_dataset: BigQuery dataset for storing results\n        destination_table: Table for storing incremental results\n        deduped_table: Table for storing deduplicated results\n        location: BigQuery location\n    \"\"\"\n    import logging\n    from datetime import datetime, timedelta\n\n    import backoff\n    import bigframes.ml.llm as llm\n    import bigframes.pandas as bpd\n    import google.api_core.exceptions\n    import swifter\n    from google.cloud import bigquery\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from markdownify import markdownify\n\n    # Initialize logging\n    logging.basicConfig(level=logging.INFO)\n    logging.info(f\"Using {swifter} for apply operations.\")\n\n    # Initialize clients\n    logging.info(\"Initializing clients...\")\n    bq_client = bigquery.Client(project=project_id, location=location)\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    logging.info(\"Clients initialized.\")\n\n    # Set date range for data fetch\n    schedule_time_dt: datetime = datetime.fromisoformat(\n        schedule_time.replace(\"Z\", \"+00:00\")\n    )\n    if schedule_time_dt.year == 1970:\n        logging.warning(\n            \"Pipeline schedule not set. Setting schedule_time to current date.\"\n        )\n        schedule_time_dt = datetime.now()\n\n    # Note: The following line sets the schedule time 5 years back to allow sample data to be present.\n    # For your use case, please comment out the following line to use the actual schedule time.\n    schedule_time_dt = schedule_time_dt - timedelta(days=5 * 365)\n\n    START_DATE: datetime = schedule_time_dt - timedelta(\n        days=look_back_days\n    )  # Start date for data processing window\n    END_DATE: datetime = schedule_time_dt  # End date for data processing window\n\n    logging.info(f\"Date range set: START_DATE={START_DATE}, END_DATE={END_DATE}\")\n\n    def fetch_stackoverflow_data(\n        dataset_suffix: str, start_date: str, end_date: str\n    ) -> bpd.DataFrame:\n        \"\"\"Fetch StackOverflow data from BigQuery.\"\"\"\n        query = f\"\"\"\n            SELECT\n                creation_date,\n                last_edit_date,\n                question_id,\n                question_title,\n                question_body AS question_text,\n                answers\n            FROM `production-ai-template.stackoverflow_qa_{dataset_suffix}.stackoverflow_python_questions_and_answers`\n            WHERE TRUE\n                {f'AND TIMESTAMP_TRUNC(creation_date, DAY) BETWEEN TIMESTAMP(\"{start_date}\") AND TIMESTAMP(\"{end_date}\")' if is_incremental else \"\"}\n        \"\"\"\n        logging.info(\"Fetching StackOverflow data from BigQuery...\")\n        return bpd.read_gbq(query)\n\n    def convert_html_to_markdown(html: str) -> str:\n        \"\"\"Convert HTML into Markdown for easier parsing and rendering after LLM response.\"\"\"\n        return markdownify(html).strip()\n\n    def create_answers_markdown(answers: list) -> str:\n        \"\"\"Convert each answer's HTML to markdown and concatenate into a single markdown text.\"\"\"\n        answers_md = \"\"\n        for index, answer_record in enumerate(answers):\n            answers_md += (\n                f\"\\n\\n## Answer {index + 1}:\\n\"  # Answer number is H2 heading size\n            )\n            answers_md += convert_html_to_markdown(answer_record[\"body\"])\n        return answers_md\n\n    def create_table_if_not_exist(\n        df: bpd.DataFrame,\n        project_id: str,\n        dataset_id: str,\n        table_id: str,\n        partition_column: str,\n        location: str = location,\n    ) -> None:\n        \"\"\"Create BigQuery table with time partitioning if it doesn't exist.\"\"\"\n        table_schema = bq_client.get_table(df.head(0).to_gbq()).schema\n        table = bigquery.Table(\n            f\"{project_id}.{dataset_id}.{table_id}\", schema=table_schema\n        )\n        table.time_partitioning = bigquery.TimePartitioning(\n            type_=bigquery.TimePartitioningType.DAY, field=partition_column\n        )\n\n        dataset = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n        dataset.location = location\n        bq_client.create_dataset(dataset, exists_ok=True)\n        bq_client.create_table(table=table, exists_ok=True)\n\n    # Fetch and preprocess data\n    logging.info(\"Fetching and preprocessing data...\")\n    df = fetch_stackoverflow_data(\n        start_date=START_DATE.strftime(\"%Y-%m-%d\"),\n        end_date=END_DATE.strftime(\"%Y-%m-%d\"),\n        dataset_suffix=location.lower().replace(\"-\", \"_\"),\n    )\n    df = (\n        df.sort_values(\"last_edit_date\", ascending=False)\n        .drop_duplicates(\"question_id\")\n        .reset_index(drop=True)\n    )\n    logging.info(\"Data fetched and preprocessed.\")\n\n    # Convert content to markdown\n    logging.info(\"Converting content to markdown...\")\n\n    # Create markdown fields efficiently\n    df[\"question_title_md\"] = (\n        \"# \" + df[\"question_title\"] + \"\\n\"\n    )  # Title is H1 heading size\n    df[\"question_text_md\"] = (\n        df[\"question_text\"].to_pandas().swifter.apply(convert_html_to_markdown) + \"\\n\"\n    )\n    df[\"answers_md\"] = df[\"answers\"].to_pandas().swifter.apply(create_answers_markdown)\n\n    # Create a column containing the whole markdown text\n    df[\"full_text_md\"] = (\n        df[\"question_title_md\"] + df[\"question_text_md\"] + df[\"answers_md\"]\n    )\n    logging.info(\"Content converted to markdown.\")\n\n    # Keep only necessary columns\n    df = df[[\"last_edit_date\", \"question_id\", \"question_text\", \"full_text_md\"]]\n\n    # Split text into chunks\n    logging.info(\"Splitting text into chunks...\")\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n    )\n\n    df[\"text_chunk\"] = (\n        df[\"full_text_md\"]\n        .to_pandas()\n        .astype(object)\n        .swifter.apply(text_splitter.split_text)\n    )\n    logging.info(\"Text split into chunks.\")\n\n    # Create chunk IDs and explode chunks into rows\n    logging.info(\"Creating chunk IDs and exploding chunks into rows...\")\n    chunk_ids = [\n        str(idx) for text_chunk in df[\"text_chunk\"] for idx in range(len(text_chunk))\n    ]\n    df = df.explode(\"text_chunk\").reset_index(drop=True)\n    df[\"chunk_id\"] = df[\"question_id\"].astype(\"string\") + \"__\" + chunk_ids\n    logging.info(\"Chunk IDs created and chunks exploded.\")\n\n    # Generate embeddings\n    logging.info(\"Generating embeddings...\")\n\n    # The first invocation in a new project might fail due to permission propagation.\n    @backoff.on_exception(\n        backoff.expo, google.api_core.exceptions.InvalidArgument, max_tries=10\n    )\n    def create_embedder() -> llm.TextEmbeddingGenerator:\n        return llm.TextEmbeddingGenerator(model_name=\"text-embedding-005\")\n\n    embedder = create_embedder()\n\n    embeddings_df = embedder.predict(df[\"text_chunk\"])\n    logging.info(\"Embeddings generated.\")\n\n    df = df.assign(\n        embedding=embeddings_df[\"ml_generate_embedding_result\"],\n        embedding_statistics=embeddings_df[\"ml_generate_embedding_statistics\"],\n        embedding_status=embeddings_df[\"ml_generate_embedding_status\"],\n        creation_timestamp=datetime.now(),\n    )\n\n    # Store results in BigQuery\n    PARTITION_DATE_COLUMN = \"creation_timestamp\"\n\n    # Create and populate incremental table\n    logging.info(\"Creating and populating incremental table...\")\n    create_table_if_not_exist(\n        df=df,\n        project_id=project_id,\n        dataset_id=destination_dataset,\n        table_id=destination_table,\n        partition_column=PARTITION_DATE_COLUMN,\n    )\n\n    if_exists_mode = \"append\" if is_incremental else \"replace\"\n    df.to_gbq(\n        destination_table=f\"{destination_dataset}.{destination_table}\",\n        if_exists=if_exists_mode,\n    )\n    logging.info(\"Incremental table created and populated.\")\n\n    # Create deduplicated table\n    logging.info(\"Creating deduplicated table...\")\n    df_questions = bpd.read_gbq(\n        f\"{destination_dataset}.{destination_table}\", use_cache=False\n    )\n    max_date_df = (\n        df_questions.groupby(\"question_id\")[\"creation_timestamp\"].max().reset_index()\n    )\n    df_questions_dedup = max_date_df.merge(\n        df_questions, how=\"inner\", on=[\"question_id\", \"creation_timestamp\"]\n    )\n\n    create_table_if_not_exist(\n        df=df_questions_dedup,\n        project_id=project_id,\n        dataset_id=destination_dataset,\n        table_id=deduped_table,\n        partition_column=PARTITION_DATE_COLUMN,\n    )\n\n    df_questions_dedup.to_gbq(\n        destination_table=f\"{destination_dataset}.{deduped_table}\",\n        if_exists=\"replace\",\n    )\n    logging.info(\"Deduplicated table created and populated.\")\n    # Set artifact metadata (important!)\n    output_table.uri = (\n        f\"bq://{project_id}.{destination_dataset}.{deduped_table}\"  # Full BQ URI\n    )\n    output_table.metadata[\"projectId\"] = project_id\n    output_table.metadata[\"datasetId\"] = destination_dataset\n    output_table.metadata[\"tableId\"] = deduped_table\n\n"
          ],
          "image": "us-docker.pkg.dev/production-ai-template/starter-pack/data_processing:0.2"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "A pipeline to run ingestion of new data into the datastore",
    "name": "pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "ingest-data": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-ingest-data"
          },
          "dependentTasks": [
            "process-data"
          ],
          "inputs": {
            "artifacts": {
              "input_table": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "output_table",
                  "producerTask": "process-data"
                }
              }
            },
            "parameters": {
              "ingestion_batch_size": {
                "componentInputParameter": "ingestion_batch_size"
              },
              "is_incremental": {
                "runtimeValue": {
                  "constant": false
                }
              },
              "location": {
                "componentInputParameter": "location"
              },
              "look_back_days": {
                "componentInputParameter": "look_back_days"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "schedule_time": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_schedule_time_utc}}"
                }
              },
              "vector_search_data_bucket_name": {
                "componentInputParameter": "vector_search_data_bucket_name"
              },
              "vector_search_index": {
                "componentInputParameter": "vector_search_index"
              },
              "vector_search_index_endpoint": {
                "componentInputParameter": "vector_search_index_endpoint"
              }
            }
          },
          "retryPolicy": {
            "backoffDuration": "0s",
            "backoffFactor": 2.0,
            "backoffMaxDuration": "3600s",
            "maxRetryCount": 2
          },
          "taskInfo": {
            "name": "ingest-data"
          }
        },
        "process-data": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-process-data"
          },
          "inputs": {
            "parameters": {
              "chunk_overlap": {
                "componentInputParameter": "chunk_overlap"
              },
              "chunk_size": {
                "componentInputParameter": "chunk_size"
              },
              "deduped_table": {
                "componentInputParameter": "deduped_table"
              },
              "destination_dataset": {
                "componentInputParameter": "destination_dataset"
              },
              "destination_table": {
                "componentInputParameter": "destination_table"
              },
              "is_incremental": {
                "componentInputParameter": "is_incremental"
              },
              "location": {
                "componentInputParameter": "location"
              },
              "look_back_days": {
                "componentInputParameter": "look_back_days"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "schedule_time": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_schedule_time_utc}}"
                }
              }
            }
          },
          "retryPolicy": {
            "backoffDuration": "0s",
            "backoffFactor": 2.0,
            "backoffMaxDuration": "3600s",
            "maxRetryCount": 2
          },
          "taskInfo": {
            "name": "process-data"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "chunk_overlap": {
          "defaultValue": 20.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "chunk_size": {
          "defaultValue": 1500.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "deduped_table": {
          "defaultValue": "questions_embeddings",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "destination_dataset": {
          "defaultValue": "my_rag_agent_from_gcs_stackoverflow_data",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "destination_table": {
          "defaultValue": "incremental_questions_embeddings",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "ingestion_batch_size": {
          "defaultValue": 1000.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "is_incremental": {
          "defaultValue": true,
          "isOptional": true,
          "parameterType": "BOOLEAN"
        },
        "location": {
          "parameterType": "STRING"
        },
        "look_back_days": {
          "defaultValue": 1.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "project_id": {
          "parameterType": "STRING"
        },
        "vector_search_data_bucket_name": {
          "defaultValue": "",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vector_search_index": {
          "defaultValue": "",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vector_search_index_endpoint": {
          "defaultValue": "",
          "isOptional": true,
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.11.0"
}